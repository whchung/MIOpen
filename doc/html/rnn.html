

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Recurrent Neural Networks &mdash; MIOpen: AMD&#39;s deep learning library</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="MIOpen: AMD&#39;s deep learning library" href="index.html"/>
        <link rel="up" title="API Reference" href="apireference.html"/>
        <link rel="next" title="Batch Normalization Layer" href="batchnorm.html"/>
        <link rel="prev" title="Convolutional Layer" href="convolution.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> MIOpen
          

          
          </a>

          
            
            
              <div class="version">
                2.1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="releasenotes.html">MIOpen Release notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">09/25/2019 [2.1.0]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">08/13/2019 [ 2.0.1 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">07/08/2019 [ 2.0.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">05/03/2019 [ 1.8.1 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">04/11/2019 [ 1.8.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">02/06/2019 [ 1.7.1 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">12/19/2018 [ 1.7.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">11/18/2018 [ 1.6.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">09/14/2018 [ 1.5.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">07/30/2018 [ 1.4.2 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">07/19/2018 [ 1.4.1 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">07/06/2018 [ 1.4.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">03/30/2018 [ 1.3.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">03/08/2018 [ 1.2.1 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">12/15/2017 [ 1.2.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">09/08/2017 [ 1.1.0 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">08/27/2017 [ 1.0.2 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#">07/26/2017 [ 1.0.1 ]</a></li>
<li class="toctree-l2"><a class="reference internal" href="releasenotes.html#initial-release">06/30/2017 [ 1.0.0 ] Initial release</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Build and Install Instructions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-miopen-with-pre-built-packages">Installing MIOpen with pre-built packages</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-the-dependencies">Installing the dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-minimum-dependencies-in-rocm-environment">Installing minimum dependencies in ROCm environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#building-miopen-from-source">Building MIOpen from source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="install.html#configuring-with-cmake">Configuring with cmake</a></li>
<li class="toctree-l3"><a class="reference internal" href="install.html#for-opencl-run">For OpenCL, run:</a></li>
<li class="toctree-l3"><a class="reference internal" href="install.html#for-hip-run">For HIP, run:</a></li>
<li class="toctree-l3"><a class="reference internal" href="install.html#setting-up-locations">Setting Up Locations</a></li>
<li class="toctree-l3"><a class="reference internal" href="install.html#system-performance-database-and-user-database">System Performance Database and User Database</a></li>
<li class="toctree-l3"><a class="reference internal" href="install.html#persistent-program-cache">Persistent Program Cache</a></li>
<li class="toctree-l3"><a class="reference internal" href="install.html#changing-the-cmake-configuration">Changing the cmake configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="install.html#building-the-library">Building the library</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#building-the-driver">Building the driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#running-the-tests">Running the tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#building-the-documentation">Building the documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#formatting-the-code">Formatting the code</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#installing-the-dependencies-manually">Installing the dependencies manually</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#using-docker">Using docker</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="driver.html">MIOpenDriver</a><ul>
<li class="toctree-l2"><a class="reference internal" href="driver.html#building-the-driver">Building the Driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="driver.html#base-arguments">Base Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="driver.html#executing-miopendriver">Executing MIOpenDriver</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="DebugAndLogging.html">Debugging and Logging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="DebugAndLogging.html#logging">Logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="DebugAndLogging.html#layer-filtering">Layer Filtering</a></li>
<li class="toctree-l2"><a class="reference internal" href="DebugAndLogging.html#rocblas-logging-and-behavior">rocBlas Logging and Behavior</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cache.html">Kernel Cache</a><ul>
<li class="toctree-l2"><a class="reference internal" href="cache.html#clear-the-cache">Clear the cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="cache.html#disabling-the-cache">Disabling the cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="cache.html#updating-miopen-and-removing-the-cache">Updating MIOpen and removing the cache</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="perfdatabase.html">Performance Database</a><ul>
<li class="toctree-l2"><a class="reference internal" href="perfdatabase.html#auto-tuning-the-kernels">Auto-tuning the kernels.</a><ul>
<li class="toctree-l3"><a class="reference internal" href="perfdatabase.html#miopen-find-enforce">MIOPEN_FIND_ENFORCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="perfdatabase.html#miopen-find-enforce-scope">MIOPEN_FIND_ENFORCE_SCOPE</a></li>
<li class="toctree-l3"><a class="reference internal" href="perfdatabase.html#updating-miopen-and-the-user-db">Updating MIOpen and the User Db</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="finddb.html">Find-Db Database</a><ul>
<li class="toctree-l2"><a class="reference internal" href="finddb.html#populating-the-user-find-db">Populating the User Find-Db</a></li>
<li class="toctree-l2"><a class="reference internal" href="finddb.html#updating-miopen-and-the-user-find-db">Updating MIOpen and the User Find-Db</a></li>
<li class="toctree-l2"><a class="reference internal" href="finddb.html#disabling-find-db">Disabling Find-Db</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="find_and_immediate.html">Find and Immediate Mode</a><ul>
<li class="toctree-l2"><a class="reference internal" href="find_and_immediate.html#find-api">Find API</a></li>
<li class="toctree-l2"><a class="reference internal" href="find_and_immediate.html#immediate-mode-api">Immediate Mode API</a></li>
<li class="toctree-l2"><a class="reference internal" href="find_and_immediate.html#immediate-mode-fall-back">Immediate Mode Fall Back</a></li>
<li class="toctree-l2"><a class="reference internal" href="find_and_immediate.html#limitations-of-immediate-mode">Limitations of Immediate Mode</a><ul>
<li class="toctree-l3"><a class="reference internal" href="find_and_immediate.html#architectual-limitations">Architectual Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="find_and_immediate.html#backend-limitations">Backend Limitations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Getting_Started_FusionAPI.html">Fusion API: Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="Getting_Started_FusionAPI.html#intended-audience">Intended Audience</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#create-a-fusion-plan">Create a Fusion Plan</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#create-and-add-operators">Create and add Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#compile-the-fusion-plan">Compile the Fusion Plan</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#set-the-runtime-arguments">Set the runtime arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#execute-a-fusion-plan">Execute a Fusion Plan</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#cleanup">Cleanup</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#supported-fusions">Supported Fusions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Getting_Started_FusionAPI.html#performance-comparison-to-non-fused-kernels">Performance Comparison to Non-Fused Kernels</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="apireference.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="datatypes.html">Datatypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="handle.html">Handle</a><ul>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopenstatus-t">miopenStatus_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopencreate">miopenCreate</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopencreatewithstream">miopenCreateWithStream</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopendestroy">miopenDestroy</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopensetstream">miopenSetStream</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopengetstream">miopenGetStream</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopengetkerneltime">miopenGetKernelTime</a></li>
<li class="toctree-l3"><a class="reference internal" href="handle.html#miopenenableprofiling">miopenEnableProfiling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html">Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopendatatype-t">miopenDataType_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopentensorop-t">miopenTensorOp_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopencreatetensordescriptor">miopenCreateTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopenset4dtensordescriptor">miopenSet4dTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopenget4dtensordescriptor">miopenGet4dTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopensettensordescriptor">miopenSetTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopengettensordescriptorsize">miopenGetTensorDescriptorSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopengettensordescriptor">miopenGetTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopendestroytensordescriptor">miopenDestroyTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopenoptensor">miopenOpTensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopensettensor">miopenSetTensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensor.html#miopenscaletensor">miopenScaleTensor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="activation.html">Activation Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopenactivationmode-t">miopenActivationMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopencreateactivationdescriptor">miopenCreateActivationDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopensetactivationdescriptor">miopenSetActivationDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopengetactivationdescriptor">miopenGetActivationDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopenactivationforward">miopenActivationForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopenactivationbackward">miopenActivationBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="activation.html#miopendestroyactivationdescriptor">miopenDestroyActivationDescriptor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="convolution.html">Convolutional Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionmode-t">miopenConvolutionMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvfwdalgorithm-t">miopenConvFwdAlgorithm_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvbwdweightsalgorithm-t">miopenConvBwdWeightsAlgorithm_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvbwddataalgorithm-t">miopenConvBwdDataAlgorithm_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvalgoperf-t">miopenConvAlgoPerf_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopencreateconvolutiondescriptor">miopenCreateConvolutionDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopeninitconvolutiondescriptor">miopenInitConvolutionDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopeninitconvolutionnddescriptor">miopenInitConvolutionNdDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopengetconvolutiondescriptor">miopenGetConvolutionDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopengetconvolutionnddescriptor">miopenGetConvolutionNdDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopensetconvolutiongroupcount">miopenSetConvolutionGroupCount</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopensettransposeconvoutputpadding">miopenSetTransposeConvOutputPadding</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopensettransposeconvndoutputpadding">miopenSetTransposeConvNdOutputPadding</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopengetconvolutionforwardoutputdim">miopenGetConvolutionForwardOutputDim</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopengetconvolutionndforwardoutputdim">miopenGetConvolutionNdForwardOutputDim</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionforwardgetworkspacesize">miopenConvolutionForwardGetWorkSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenfindconvolutionforwardalgorithm">miopenFindConvolutionForwardAlgorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionforward">miopenConvolutionForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionforwardbias">miopenConvolutionForwardBias</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenfindconvolutionbackwarddataalgorithm">miopenFindConvolutionBackwardDataAlgorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionbackwarddata">miopenConvolutionBackwardData</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionbackwarddatagetworkspacesize">miopenConvolutionBackwardDataGetWorkSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionbackwardweightsgetworkspacesize">miopenConvolutionBackwardWeightsGetWorkSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenfindconvolutionbackwardweightsalgorithm">miopenFindConvolutionBackwardWeightsAlgorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionbackwardweights">miopenConvolutionBackwardWeights</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopenconvolutionbackwardbias">miopenConvolutionBackwardBias</a></li>
<li class="toctree-l3"><a class="reference internal" href="convolution.html#miopendestroyconvolutiondescriptor">miopenDestroyConvolutionDescriptor</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Recurrent Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnmode-t">miopenRNNMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnninputmode-t">miopenRNNInputMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnalgo-t">miopenRNNAlgo_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnndirectionmode-t">miopenRNNDirectionMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnbiasmode-t">miopenRNNBiasMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnngemmalgomode-t">miopenRNNGEMMalgoMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopencreaternndescriptor">miopenCreateRNNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnndescriptor">miopenGetRNNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopendestroyrnndescriptor">miopenDestroyRNNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopensetrnndescriptor">miopenSetRNNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnworkspacesize">miopenGetRNNWorkspaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnntrainingreservesize">miopenGetRNNTrainingReserveSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnparamssize">miopenGetRNNParamsSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnparamsdescriptor">miopenGetRNNParamsDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnninputtensorsize">miopenGetRNNInputTensorSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnhiddentensorsize">miopenGetRNNHiddenTensorSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnlayerparamsize">miopenGetRNNLayerParamSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnlayerbiassize">miopenGetRNNLayerBiasSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnlayerparam">miopenGetRNNLayerParam</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnlayerbias">miopenGetRNNLayerBias</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopensetrnnlayerparam">miopenSetRNNLayerParam</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopensetrnnlayerbias">miopenSetRNNLayerBias</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnlayerparamoffset">miopenGetRNNLayerParamOffset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopengetrnnlayerbiasoffset">miopenGetRNNLayerBiasOffset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnforwardtraining">miopenRNNForwardTraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnbackwarddata">miopenRNNBackwardData</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnbackwardweights">miopenRNNBackwardWeights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#miopenrnnforwardinference">miopenRNNForwardInference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="batchnorm.html">Batch Normalization Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="batchnorm.html#miopenbatchnormmode-t">miopenBatchNormMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="batchnorm.html#miopenderivebntensordescriptor">miopenDeriveBNTensorDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="batchnorm.html#miopenbatchnormalizationforwardtraining">miopenBatchNormalizationForwardTraining</a></li>
<li class="toctree-l3"><a class="reference internal" href="batchnorm.html#miopenbatchnormalizationforwardinference">miopenBatchNormalizationForwardInference</a></li>
<li class="toctree-l3"><a class="reference internal" href="batchnorm.html#miopenbatchnormalizationbackward">miopenBatchNormalizationBackward</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="lrn.html">Local Response Normalization Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopenlrnmode-t">miopenLRNMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopencreatelrndescriptor">miopenCreateLRNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopensetlrndescriptor">miopenSetLRNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopengetlrndescriptor">miopenGetLRNDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopenlrngetworkspacesize">miopenLRNGetWorkSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopenlrnforward">miopenLRNForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopenlrnbackward">miopenLRNBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="lrn.html#miopendestroylrndescriptor">miopenDestroyLRNDescriptor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pooling.html">Pooling Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenpoolingmode-t">miopenPoolingMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenindextype-t">miopenIndexType_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopencreatepoolingdescriptor">miopenCreatePoolingDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenset2dpoolingdescriptor">miopenSet2dPoolingDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopensetpoolingindextype">miopenSetPoolingIndexType</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopengetpoolingindextype">miopenGetPoolingIndexType</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenget2dpoolingdescriptor">miopenGet2dPoolingDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopengetpoolingforwardoutputdim">miopenGetPoolingForwardOutputDim</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenpoolinggetworkspacesize">miopenPoolingGetWorkSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenpoolinggetworkspacesizev2">miopenPoolingGetWorkSpaceSizeV2</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenpoolingforward">miopenPoolingForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopenpoolingbackward">miopenPoolingBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="pooling.html#miopendestroypoolingdescriptor">miopenDestroyPoolingDescriptor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="softmax.html">Softmax Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="softmax.html#miopensoftmaxalgorithm-t">miopenSoftmaxAlgorithm_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="softmax.html#miopensoftmaxmode-t">miopenSoftmaxMode_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="softmax.html#miopensoftmaxforward">miopenSoftmaxForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="softmax.html#miopensoftmaxbackward">miopenSoftmaxBackward</a></li>
<li class="toctree-l3"><a class="reference internal" href="softmax.html#miopensoftmaxforward-v2">miopenSoftmaxForward_V2</a></li>
<li class="toctree-l3"><a class="reference internal" href="softmax.html#miopensoftmaxbackward-v2">miopenSoftmaxBackward_V2</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="fusion.html">Layer Fusion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopenfusiondirection-t">miopenFusionDirection_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencreatefusionplan">miopenCreateFusionPlan</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopendestroyfusionplan">miopenDestroyFusionPlan</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencompilefusionplan">miopenCompileFusionPlan</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopenfusionplangetop">miopenFusionPlanGetOp</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopenfusionplangetworkspacesize">miopenFusionPlanGetWorkSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopenfusionplanconvolutiongetalgo">miopenFusionPlanConvolutionGetAlgo</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencreateopconvforward">miopenCreateOpConvForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencreateopactivationforward">miopenCreateOpActivationForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencreateopbiasforward">miopenCreateOpBiasForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencreateopbatchnorminference">miopenCreateOpBatchNormInference</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopencreateoperatorargs">miopenCreateOperatorArgs</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopendestroyoperatorargs">miopenDestroyOperatorArgs</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopensetopargsconvforward">miopenSetOpArgsConvForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopensetopargsbatchnorminference">miopenSetOpArgsBatchNormInference</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopensetopargsbiasforward">miopenSetOpArgsBiasForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="fusion.html#miopenexecutefusionplan">miopenExecuteFusionPlan</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="loss.html">Loss Function Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopenctclossalgo-t">miopenCTCLossAlgo_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopencreatectclossdescriptor">miopenCreateCTCLossDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopengetctclossdescriptor">miopenGetCTCLossDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopendestroyctclossdescriptor">miopenDestroyCTCLossDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopensetctclossdescriptor">miopenSetCTCLossDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopengetctclossworkspacesize">miopenGetCTCLossWorkspaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="loss.html#miopenctcloss">miopenCTCLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dropout.html">Dropout Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopenrngtype-t">miopenRNGType_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopencreatedropoutdescriptor">miopenCreateDropoutDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopengetdropoutdescriptor">miopenGetDropoutDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopenrestoredropoutdescriptor">miopenRestoreDropoutDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopendestroydropoutdescriptor">miopenDestroyDropoutDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopensetdropoutdescriptor">miopenSetDropoutDescriptor</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopendropoutgetreservespacesize">miopenDropoutGetReserveSpaceSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopendropoutgetstatessize">miopenDropoutGetStatesSize</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopendropoutforward">miopenDropoutForward</a></li>
<li class="toctree-l3"><a class="reference internal" href="dropout.html#miopendropoutbackward">miopenDropoutBackward</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MIOpen</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
          <li><a href="apireference.html">API Reference</a> &raquo;</li>
        
      <li>Recurrent Neural Networks</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/rnn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="recurrent-neural-networks">
<h1>Recurrent Neural Networks<a class="headerlink" href="#recurrent-neural-networks" title="Permalink to this headline">¶</a></h1>
<div class="section" id="miopenrnnmode-t">
<h2>miopenRNNMode_t<a class="headerlink" href="#miopenrnnmode-t" title="Permalink to this headline">¶</a></h2>
<dl class="type">
<dt id="_CPPv315miopenRNNMode_t">
<span id="_CPPv215miopenRNNMode_t"></span><span id="miopenRNNMode_t"></span><span class="target" id="group__RNN_1ga016f266507f199def908fe39c43d7877"></span><em class="property">enum </em><code class="descname">miopenRNNMode_t</code><a class="headerlink" href="#_CPPv315miopenRNNMode_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>RNN mode selection for rnn layer preference </p>
<p><em>Values:</em></p>
<dl class="member">
<dt id="_CPPv313miopenRNNRELU">
<span id="_CPPv213miopenRNNRELU"></span><span id="miopenRNNRELU"></span><span class="target" id="group__RNN_1gga016f266507f199def908fe39c43d7877af714eb36c96ca365b643e7e8417c10cc"></span><code class="descname">miopenRNNRELU</code> = 0<a class="headerlink" href="#_CPPv313miopenRNNRELU" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>RNN with ReLU activation </p>
</dd></dl>

<dl class="member">
<dt id="_CPPv313miopenRNNTANH">
<span id="_CPPv213miopenRNNTANH"></span><span id="miopenRNNTANH"></span><span class="target" id="group__RNN_1gga016f266507f199def908fe39c43d7877a1d43e2e3151aa1266cc10e8623c0a32b"></span><code class="descname">miopenRNNTANH</code> = 1<a class="headerlink" href="#_CPPv313miopenRNNTANH" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>RNN with tanh activation </p>
</dd></dl>

<dl class="member">
<dt id="_CPPv310miopenLSTM">
<span id="_CPPv210miopenLSTM"></span><span id="miopenLSTM"></span><span class="target" id="group__RNN_1gga016f266507f199def908fe39c43d7877a97804b8e078f16b327e50e5554df970c"></span><code class="descname">miopenLSTM</code> = 2<a class="headerlink" href="#_CPPv310miopenLSTM" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>LSTM </p>
</dd></dl>

<dl class="member">
<dt id="_CPPv39miopenGRU">
<span id="_CPPv29miopenGRU"></span><span id="miopenGRU"></span><span class="target" id="group__RNN_1gga016f266507f199def908fe39c43d7877aa13bc340d91e98e610e92b75e5928a66"></span><code class="descname">miopenGRU</code> = 3<a class="headerlink" href="#_CPPv39miopenGRU" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>GRU </p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="miopenrnninputmode-t">
<h2>miopenRNNInputMode_t<a class="headerlink" href="#miopenrnninputmode-t" title="Permalink to this headline">¶</a></h2>
<dl class="type">
<dt id="_CPPv320miopenRNNInputMode_t">
<span id="_CPPv220miopenRNNInputMode_t"></span><span id="miopenRNNInputMode_t"></span><span class="target" id="group__RNN_1ga11808e1b616d9b9d7e6c701986783af7"></span><em class="property">enum </em><code class="descname">miopenRNNInputMode_t</code><a class="headerlink" href="#_CPPv320miopenRNNInputMode_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Recurrent Neural Network layer initial input mode </p>
<p><em>Values:</em></p>
<dl class="member">
<dt id="_CPPv315miopenRNNlinear">
<span id="_CPPv215miopenRNNlinear"></span><span id="miopenRNNlinear"></span><span class="target" id="group__RNN_1gga11808e1b616d9b9d7e6c701986783af7a168f261ee3dc35ea3fe636c644610c2f"></span><code class="descname">miopenRNNlinear</code> = 0<a class="headerlink" href="#_CPPv315miopenRNNlinear" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Matrix multiplication at the input of the first layer </p>
</dd></dl>

<dl class="member">
<dt id="_CPPv313miopenRNNskip">
<span id="_CPPv213miopenRNNskip"></span><span id="miopenRNNskip"></span><span class="target" id="group__RNN_1gga11808e1b616d9b9d7e6c701986783af7a99c1caff2a69fb37d964fb3692c989da"></span><code class="descname">miopenRNNskip</code> = 1<a class="headerlink" href="#_CPPv313miopenRNNskip" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>No operation is performed at the input of the first layer. </p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="miopenrnnalgo-t">
<h2>miopenRNNAlgo_t<a class="headerlink" href="#miopenrnnalgo-t" title="Permalink to this headline">¶</a></h2>
<dl class="type">
<dt id="_CPPv315miopenRNNAlgo_t">
<span id="_CPPv215miopenRNNAlgo_t"></span><span id="miopenRNNAlgo_t"></span><span class="target" id="group__RNN_1ga6bca6bf2c239cb387d99a07cb6b331c4"></span><em class="property">enum </em><code class="descname">miopenRNNAlgo_t</code><a class="headerlink" href="#_CPPv315miopenRNNAlgo_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Recurrent Neural Network algorithm mode </p>
<p><em>Values:</em></p>
<dl class="member">
<dt id="_CPPv316miopenRNNdefault">
<span id="_CPPv216miopenRNNdefault"></span><span id="miopenRNNdefault"></span><span class="target" id="group__RNN_1gga6bca6bf2c239cb387d99a07cb6b331c4aee4782e7cebfb009314cdd6c695a5b90"></span><code class="descname">miopenRNNdefault</code> = 0<a class="headerlink" href="#_CPPv316miopenRNNdefault" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Supported </p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="miopenrnndirectionmode-t">
<h2>miopenRNNDirectionMode_t<a class="headerlink" href="#miopenrnndirectionmode-t" title="Permalink to this headline">¶</a></h2>
<dl class="type">
<dt id="_CPPv324miopenRNNDirectionMode_t">
<span id="_CPPv224miopenRNNDirectionMode_t"></span><span id="miopenRNNDirectionMode_t"></span><span class="target" id="group__RNN_1ga3c7adae8941033d266f1d5e029504c38"></span><em class="property">enum </em><code class="descname">miopenRNNDirectionMode_t</code><a class="headerlink" href="#_CPPv324miopenRNNDirectionMode_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Recurrent Neural Network bi-directional behavior </p>
<p><em>Values:</em></p>
<dl class="member">
<dt id="_CPPv321miopenRNNunidirection">
<span id="_CPPv221miopenRNNunidirection"></span><span id="miopenRNNunidirection"></span><span class="target" id="group__RNN_1gga3c7adae8941033d266f1d5e029504c38a78752802fd2c7248fd4fdddbf613264b"></span><code class="descname">miopenRNNunidirection</code> = 0<a class="headerlink" href="#_CPPv321miopenRNNunidirection" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Forward in time only. </p>
</dd></dl>

<dl class="member">
<dt id="_CPPv320miopenRNNbidirection">
<span id="_CPPv220miopenRNNbidirection"></span><span id="miopenRNNbidirection"></span><span class="target" id="group__RNN_1gga3c7adae8941033d266f1d5e029504c38a2f0f99690655d0df5ca16bd5011908ea"></span><code class="descname">miopenRNNbidirection</code> = 1<a class="headerlink" href="#_CPPv320miopenRNNbidirection" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Forward and backwards in time. </p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="miopenrnnbiasmode-t">
<h2>miopenRNNBiasMode_t<a class="headerlink" href="#miopenrnnbiasmode-t" title="Permalink to this headline">¶</a></h2>
<dl class="type">
<dt id="_CPPv319miopenRNNBiasMode_t">
<span id="_CPPv219miopenRNNBiasMode_t"></span><span id="miopenRNNBiasMode_t"></span><span class="target" id="group__RNN_1ga47b037e570937a567de38e8898a99f37"></span><em class="property">enum </em><code class="descname">miopenRNNBiasMode_t</code><a class="headerlink" href="#_CPPv319miopenRNNBiasMode_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Recurrent Neural Network add on bias </p>
<p><em>Values:</em></p>
<dl class="member">
<dt id="_CPPv315miopenRNNNoBias">
<span id="_CPPv215miopenRNNNoBias"></span><span id="miopenRNNNoBias"></span><span class="target" id="group__RNN_1gga47b037e570937a567de38e8898a99f37a2eb8172730ba33866564865fe4e2d7ea"></span><code class="descname">miopenRNNNoBias</code> = 0<a class="headerlink" href="#_CPPv315miopenRNNNoBias" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>No Biases will be applied to GEMM operations </p>
</dd></dl>

<dl class="member">
<dt id="_CPPv317miopenRNNwithBias">
<span id="_CPPv217miopenRNNwithBias"></span><span id="miopenRNNwithBias"></span><span class="target" id="group__RNN_1gga47b037e570937a567de38e8898a99f37a14fd5be6ddb03ef2d81d27ff8a868d10"></span><code class="descname">miopenRNNwithBias</code> = 1<a class="headerlink" href="#_CPPv317miopenRNNwithBias" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Biases will be applied to GEMM operations </p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="miopenrnngemmalgomode-t">
<h2>miopenRNNGEMMalgoMode_t<a class="headerlink" href="#miopenrnngemmalgomode-t" title="Permalink to this headline">¶</a></h2>
<dl class="type">
<dt id="_CPPv323miopenRNNGEMMalgoMode_t">
<span id="_CPPv223miopenRNNGEMMalgoMode_t"></span><span id="miopenRNNGEMMalgoMode_t"></span><span class="target" id="group__RNN_1gac7f800028b5634cb08aa191fa6ee0d2a"></span><em class="property">enum </em><code class="descname">miopenRNNGEMMalgoMode_t</code><a class="headerlink" href="#_CPPv323miopenRNNGEMMalgoMode_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Recurrent Neural Network add on bias </p>
<p><em>Values:</em></p>
<dl class="member">
<dt id="_CPPv317miopenRNNAlgoGEMM">
<span id="_CPPv217miopenRNNAlgoGEMM"></span><span id="miopenRNNAlgoGEMM"></span><span class="target" id="group__RNN_1ggac7f800028b5634cb08aa191fa6ee0d2aa5803419df2c12a2ea02b7560a54ebee7"></span><code class="descname">miopenRNNAlgoGEMM</code> = 0<a class="headerlink" href="#_CPPv317miopenRNNAlgoGEMM" title="Permalink to this definition">¶</a><br /></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="miopencreaternndescriptor">
<h2>miopenCreateRNNDescriptor<a class="headerlink" href="#miopencreaternndescriptor" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv325miopenCreateRNNDescriptorP21miopenRNNDescriptor_t">
<span id="_CPPv225miopenCreateRNNDescriptorP21miopenRNNDescriptor_t"></span><span id="miopenCreateRNNDescriptor__miopenRNNDescriptor_tP"></span><span class="target" id="group__RNN_1gaecffedf9767852a0ac22b4459f1dbb02"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenCreateRNNDescriptor</code><span class="sig-paren">(</span>miopenRNNDescriptor_t *<em>rnnDesc</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv325miopenCreateRNNDescriptorP21miopenRNNDescriptor_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Create a RNN layer Descriptor. </p>
<p>API for creating an uninitialized RNN layer descriptor. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: Pointer to a tensor descriptor type </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnndescriptor">
<h2>miopenGetRNNDescriptor<a class="headerlink" href="#miopengetrnndescriptor" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv322miopenGetRNNDescriptor21miopenRNNDescriptor_tP15miopenRNNMode_tP15miopenRNNAlgo_tP20miopenRNNInputMode_tP24miopenRNNDirectionMode_tP19miopenRNNBiasMode_tPiPi">
<span id="_CPPv222miopenGetRNNDescriptor21miopenRNNDescriptor_tP15miopenRNNMode_tP15miopenRNNAlgo_tP20miopenRNNInputMode_tP24miopenRNNDirectionMode_tP19miopenRNNBiasMode_tPiPi"></span><span id="miopenGetRNNDescriptor__miopenRNNDescriptor_t.miopenRNNMode_tP.miopenRNNAlgo_tP.miopenRNNInputMode_tP.miopenRNNDirectionMode_tP.miopenRNNBiasMode_tP.iP.iP"></span><span class="target" id="group__RNN_1ga9061d8324be32ddc2e609721bbff2f22"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNDescriptor</code><span class="sig-paren">(</span>miopenRNNDescriptor_t <em>rnnDesc</em>, <a class="reference internal" href="#_CPPv315miopenRNNMode_t" title="miopenRNNMode_t">miopenRNNMode_t</a> *<em>rnnMode</em>, <a class="reference internal" href="#_CPPv315miopenRNNAlgo_t" title="miopenRNNAlgo_t">miopenRNNAlgo_t</a> *<em>algoMode</em>, <a class="reference internal" href="#_CPPv320miopenRNNInputMode_t" title="miopenRNNInputMode_t">miopenRNNInputMode_t</a> *<em>inputMode</em>, <a class="reference internal" href="#_CPPv324miopenRNNDirectionMode_t" title="miopenRNNDirectionMode_t">miopenRNNDirectionMode_t</a> *<em>dirMode</em>, <a class="reference internal" href="#_CPPv319miopenRNNBiasMode_t" title="miopenRNNBiasMode_t">miopenRNNBiasMode_t</a> *<em>biasMode</em>, int *<em>hiddenSize</em>, int *<em>layer</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv322miopenGetRNNDescriptor21miopenRNNDescriptor_tP15miopenRNNMode_tP15miopenRNNAlgo_tP20miopenRNNInputMode_tP24miopenRNNDirectionMode_tP19miopenRNNBiasMode_tPiPi" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Retrieves a RNN layer descriptor’s details. </p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnMode</span></code>: RNN mode (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">algoMode</span></code>: RNN algorithm mode (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">inputMode</span></code>: RNN data input mode (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">dirMode</span></code>: Uni or bi direction mode (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasMode</span></code>: Bias used (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">hiddenSize</span></code>: Size of hidden state (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: Number of stacked layers (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopendestroyrnndescriptor">
<h2>miopenDestroyRNNDescriptor<a class="headerlink" href="#miopendestroyrnndescriptor" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv326miopenDestroyRNNDescriptor21miopenRNNDescriptor_t">
<span id="_CPPv226miopenDestroyRNNDescriptor21miopenRNNDescriptor_t"></span><span id="miopenDestroyRNNDescriptor__miopenRNNDescriptor_t"></span><span class="target" id="group__RNN_1gadda9e9eb51941ffa3d2ad42441545275"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenDestroyRNNDescriptor</code><span class="sig-paren">(</span>miopenRNNDescriptor_t <em>rnnDesc</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv326miopenDestroyRNNDescriptor21miopenRNNDescriptor_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Destroys the tensor descriptor object. </p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN tensor descriptor type (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopensetrnndescriptor">
<h2>miopenSetRNNDescriptor<a class="headerlink" href="#miopensetrnndescriptor" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv322miopenSetRNNDescriptor21miopenRNNDescriptor_tKiKi20miopenRNNInputMode_t24miopenRNNDirectionMode_t15miopenRNNMode_t19miopenRNNBiasMode_t15miopenRNNAlgo_t16miopenDataType_t">
<span id="_CPPv222miopenSetRNNDescriptor21miopenRNNDescriptor_tKiKi20miopenRNNInputMode_t24miopenRNNDirectionMode_t15miopenRNNMode_t19miopenRNNBiasMode_t15miopenRNNAlgo_t16miopenDataType_t"></span><span id="miopenSetRNNDescriptor__miopenRNNDescriptor_t.iC.iC.miopenRNNInputMode_t.miopenRNNDirectionMode_t.miopenRNNMode_t.miopenRNNBiasMode_t.miopenRNNAlgo_t.miopenDataType_t"></span><span class="target" id="group__RNN_1ga65b28d0c31d6355b36af240159eb225d"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenSetRNNDescriptor</code><span class="sig-paren">(</span>miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>hsize</em>, <em class="property">const</em> int <em>nlayers</em>, <a class="reference internal" href="#_CPPv320miopenRNNInputMode_t" title="miopenRNNInputMode_t">miopenRNNInputMode_t</a> <em>inMode</em>, <a class="reference internal" href="#_CPPv324miopenRNNDirectionMode_t" title="miopenRNNDirectionMode_t">miopenRNNDirectionMode_t</a> <em>direction</em>, <a class="reference internal" href="#_CPPv315miopenRNNMode_t" title="miopenRNNMode_t">miopenRNNMode_t</a> <em>rnnMode</em>, <a class="reference internal" href="#_CPPv319miopenRNNBiasMode_t" title="miopenRNNBiasMode_t">miopenRNNBiasMode_t</a> <em>biasMode</em>, <a class="reference internal" href="#_CPPv315miopenRNNAlgo_t" title="miopenRNNAlgo_t">miopenRNNAlgo_t</a> <em>algo</em>, <a class="reference internal" href="tensor.html#_CPPv316miopenDataType_t" title="miopenDataType_t">miopenDataType_t</a> <em>dataType</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv322miopenSetRNNDescriptor21miopenRNNDescriptor_tKiKi20miopenRNNInputMode_t24miopenRNNDirectionMode_t15miopenRNNMode_t19miopenRNNBiasMode_t15miopenRNNAlgo_t16miopenDataType_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Set the details of the RNN descriptor. </p>
<p>Interface for setting the values of the RNN descriptor object. This function requires specific algorithm selection. <dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hsize</span></code>: Hidden layer size (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">nlayers</span></code>: Number of layers (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">inMode</span></code>: RNN first layer input mode (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">direction</span></code>: RNN direction (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnMode</span></code>: RNN model type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasMode</span></code>: RNN bias included (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">algo</span></code>: RNN algorithm selected (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dataType</span></code>: Only fp32 currently supported for RNNs (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnworkspacesize">
<h2>miopenGetRNNWorkspaceSize<a class="headerlink" href="#miopengetrnnworkspacesize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv325miopenGetRNNWorkspaceSize14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tP6size_t">
<span id="_CPPv225miopenGetRNNWorkspaceSize14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tP6size_t"></span><span id="miopenGetRNNWorkspaceSize__miopenHandle_t.miopenRNNDescriptor_tC.iC.miopenTensorDescriptor_tCP.sP"></span><span class="target" id="group__RNN_1ga5d8da85bf8af5e51d17bd7156bac4bf8"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNWorkspaceSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, <em class="property">const</em> miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>sequenceLen</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>xDesc</em>, size_t *<em>numBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv325miopenGetRNNWorkspaceSize14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Query the amount of memory required to execute the RNN layer. </p>
<p>This function calculates the amount of memory required to run the RNN layer given an RNN descriptor and a tensor descriptor.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">sequenceLen</span></code>: Number of iteration unrolls (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: Number of bytes required for RNN layer execution (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnntrainingreservesize">
<h2>miopenGetRNNTrainingReserveSize<a class="headerlink" href="#miopengetrnntrainingreservesize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv331miopenGetRNNTrainingReserveSize14miopenHandle_t21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tP6size_t">
<span id="_CPPv231miopenGetRNNTrainingReserveSize14miopenHandle_t21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tP6size_t"></span><span id="miopenGetRNNTrainingReserveSize__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_tCP.sP"></span><span class="target" id="group__RNN_1gab865d73b894dd71f939fead8e35b81e3"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNTrainingReserveSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>sequenceLen</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>xDesc</em>, size_t *<em>numBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv331miopenGetRNNTrainingReserveSize14miopenHandle_t21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Query the amount of memory required for RNN training. </p>
<p>This function calculates the amount of memory required to train the RNN layer given an RNN descriptor and a tensor descriptor.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">sequenceLen</span></code>: Number of iteration unrolls (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: Number of bytes required for RNN layer execution (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnparamssize">
<h2>miopenGetRNNParamsSize<a class="headerlink" href="#miopengetrnnparamssize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv322miopenGetRNNParamsSize14miopenHandle_t21miopenRNNDescriptor_t24miopenTensorDescriptor_tP6size_t16miopenDataType_t">
<span id="_CPPv222miopenGetRNNParamsSize14miopenHandle_t21miopenRNNDescriptor_t24miopenTensorDescriptor_tP6size_t16miopenDataType_t"></span><span id="miopenGetRNNParamsSize__miopenHandle_t.miopenRNNDescriptor_t.miopenTensorDescriptor_t.sP.miopenDataType_t"></span><span class="target" id="group__RNN_1gaba51c63810c1dd7bcdd2ab027dcfaaf9"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNParamsSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, miopenTensorDescriptor_t <em>xDesc</em>, size_t *<em>numBytes</em>, <a class="reference internal" href="tensor.html#_CPPv316miopenDataType_t" title="miopenDataType_t">miopenDataType_t</a> <em>dtype</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv322miopenGetRNNParamsSize14miopenHandle_t21miopenRNNDescriptor_t24miopenTensorDescriptor_tP6size_t16miopenDataType_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Query the amount of parameter memory required for RNN training. </p>
<p>This function calculates the amount of parameter memory required to train the RNN layer given an RNN descriptor and a tensor descriptor.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: Number of bytes required for RNN layer execution (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">dtype</span></code>: MIOpen data type enum (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnparamsdescriptor">
<h2>miopenGetRNNParamsDescriptor<a class="headerlink" href="#miopengetrnnparamsdescriptor" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv328miopenGetRNNParamsDescriptor14miopenHandle_t21miopenRNNDescriptor_t24miopenTensorDescriptor_t24miopenTensorDescriptor_t16miopenDataType_t">
<span id="_CPPv228miopenGetRNNParamsDescriptor14miopenHandle_t21miopenRNNDescriptor_t24miopenTensorDescriptor_t24miopenTensorDescriptor_t16miopenDataType_t"></span><span id="miopenGetRNNParamsDescriptor__miopenHandle_t.miopenRNNDescriptor_t.miopenTensorDescriptor_t.miopenTensorDescriptor_t.miopenDataType_t"></span><span class="target" id="group__RNN_1gadef7d06acbd1e65fcc448b1ad3ea9670"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNParamsDescriptor</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, miopenTensorDescriptor_t <em>xDesc</em>, miopenTensorDescriptor_t <em>wDesc</em>, <a class="reference internal" href="tensor.html#_CPPv316miopenDataType_t" title="miopenDataType_t">miopenDataType_t</a> <em>dtype</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv328miopenGetRNNParamsDescriptor14miopenHandle_t21miopenRNNDescriptor_t24miopenTensorDescriptor_t24miopenTensorDescriptor_t16miopenDataType_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Obtain a weight tensor descriptor for RNNs. </p>
<p>This function populates a weight descriptor that describes the memory layout of the weight matrix.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: Fully populated RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A previously populated tensor descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A previously allocated tensor descriptor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">dtype</span></code>: MIOpen data type enum, currently only fp32 is supported (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnninputtensorsize">
<h2>miopenGetRNNInputTensorSize<a class="headerlink" href="#miopengetrnninputtensorsize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv327miopenGetRNNInputTensorSize14miopenHandle_t21miopenRNNDescriptor_tKiP24miopenTensorDescriptor_tP6size_t">
<span id="_CPPv227miopenGetRNNInputTensorSize14miopenHandle_t21miopenRNNDescriptor_tKiP24miopenTensorDescriptor_tP6size_t"></span><span id="miopenGetRNNInputTensorSize__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_tP.sP"></span><span class="target" id="group__RNN_1ga9ccd1e8aaa76b13e5341b6261c6fb1d6"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNInputTensorSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>seqLen</em>, miopenTensorDescriptor_t *<em>xDesc</em>, size_t *<em>numBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv327miopenGetRNNInputTensorSize14miopenHandle_t21miopenRNNDescriptor_tKiP24miopenTensorDescriptor_tP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Obtain a the size in bytes of the RNN input tensor. </p>
<p>This function determines the size in bytes of the allocation needed for the input data tensor for an RNN layer. The number of bytes is derived from the array of tensor descriptors.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: Fully populated RNN layer descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">seqLen</span></code>: Number of iteration unrolls (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: Number of bytes required for input tensor (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnhiddentensorsize">
<h2>miopenGetRNNHiddenTensorSize<a class="headerlink" href="#miopengetrnnhiddentensorsize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv328miopenGetRNNHiddenTensorSize14miopenHandle_t21miopenRNNDescriptor_tKiP24miopenTensorDescriptor_tP6size_t">
<span id="_CPPv228miopenGetRNNHiddenTensorSize14miopenHandle_t21miopenRNNDescriptor_tKiP24miopenTensorDescriptor_tP6size_t"></span><span id="miopenGetRNNHiddenTensorSize__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_tP.sP"></span><span class="target" id="group__RNN_1gad8c51060afa4d55fbf67440a45638bd1"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNHiddenTensorSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>seqLen</em>, miopenTensorDescriptor_t *<em>xDesc</em>, size_t *<em>numBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv328miopenGetRNNHiddenTensorSize14miopenHandle_t21miopenRNNDescriptor_tKiP24miopenTensorDescriptor_tP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Obtain a the size in bytes of the RNN hidden tensor. </p>
<p>This function determines the size in bytes of the allocation needed for the hidden tensor over all layers</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: Fully populated RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">seqLen</span></code>: Number of iteration unrolls (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of previously populated tensor descriptors (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: Number of bytes required for input tensor (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnlayerparamsize">
<h2>miopenGetRNNLayerParamSize<a class="headerlink" href="#miopengetrnnlayerparamsize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv326miopenGetRNNLayerParamSize14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKiP6size_t">
<span id="_CPPv226miopenGetRNNLayerParamSize14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKiP6size_t"></span><span id="miopenGetRNNLayerParamSize__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.iC.sP"></span><span class="target" id="group__RNN_1gacf408b97337cd281aa7fadb8c430b8aa"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNLayerParamSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, <em class="property">const</em> int <em>paramID</em>, size_t *<em>numBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv326miopenGetRNNLayerParamSize14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKiP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Gets the number of bytes of a parameter matrix. </p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, paramID == 0 retrieves the weight matrix associated with the in input GEMM, while paramID == 1 retrieves the weight matrix associated with the hidden state GEMM.</p>
<p>For miopenLSTM paramID 0 to 3 refer to the weight matrices associated with the input GEMM, 4-7 are associated with matrices associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 4 are for the input gate.</li>
<li>paramID 1 and 5 are for the forget gate.</li>
<li>paramID 2 and 6 are for the output gate.</li>
<li>paramID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU paramID 0 to 2 refer to the weight matrix offset associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 3 are for the update gate.</li>
<li>paramID 1 and 4 are for the reset gate.</li>
<li>paramID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: The number of bytes of the layer’s parameter matrix (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnlayerbiassize">
<h2>miopenGetRNNLayerBiasSize<a class="headerlink" href="#miopengetrnnlayerbiassize" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv325miopenGetRNNLayerBiasSize14miopenHandle_t21miopenRNNDescriptor_tKiKiP6size_t">
<span id="_CPPv225miopenGetRNNLayerBiasSize14miopenHandle_t21miopenRNNDescriptor_tKiKiP6size_t"></span><span id="miopenGetRNNLayerBiasSize__miopenHandle_t.miopenRNNDescriptor_t.iC.iC.sP"></span><span class="target" id="group__RNN_1ga38d7e7c6db99645c92b608f7117e50d7"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNLayerBiasSize</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, <em class="property">const</em> int <em>biasID</em>, size_t *<em>numBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv325miopenGetRNNLayerBiasSize14miopenHandle_t21miopenRNNDescriptor_tKiKiP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Gets the number of bytes of a bias. </p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, biasID == 0 retrieves the weight matrix associated with the in input GEMM, while biasID == 1 retrieves the bias associated with the hidden state GEMM.</p>
<p>For miopenLSTM biasID 0 to 3 refer to the biases associated with the input GEMM, 4-7 are associated with biases associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 4 are for the input gate.</li>
<li>biasID 1 and 5 are for the forget gate.</li>
<li>biasID 2 and 6 are for the output gate.</li>
<li>biasID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU biasID 0 to 2 refer to the biases associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 3 are for the update gate.</li>
<li>biasID 1 and 4 are for the reset gate.</li>
<li>biasID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">numBytes</span></code>: The number of bytes of the layer’s bias (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnlayerparam">
<h2>miopenGetRNNLayerParam<a class="headerlink" href="#miopengetrnnlayerparam" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv322miopenGetRNNLayerParam14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPKvKi24miopenTensorDescriptor_tPv">
<span id="_CPPv222miopenGetRNNLayerParam14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPKvKi24miopenTensorDescriptor_tPv"></span><span id="miopenGetRNNLayerParam__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.miopenTensorDescriptor_t.voidCP.iC.miopenTensorDescriptor_t.voidP"></span><span class="target" id="group__RNN_1ga32a36f2568cb5f688768b782dcfd6b15"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNLayerParam</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, miopenTensorDescriptor_t <em>wDesc</em>, <em class="property">const</em> void *<em>w</em>, <em class="property">const</em> int <em>paramID</em>, miopenTensorDescriptor_t <em>paramDesc</em>, void *<em>layerParam</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv322miopenGetRNNLayerParam14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPKvKi24miopenTensorDescriptor_tPv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Gets a weight matrix for a specific layer in an RNN stack. </p>
<p>This function retrieves the weight matrix data for a specific layer and parameter ID and copies the data into previously allocated device memory.</p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, paramID == 0 retrieves the weight matrix associated with the in input GEMM, while paramID == 1 retrieves the weight matrix associated with the hidden state GEMM.</p>
<p>For miopenLSTM paramID 0 to 3 refer to the weight matrices associated with the input GEMM, 4-7 are associated with matrices associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 4 are for the input gate.</li>
<li>paramID 1 and 5 are for the forget gate.</li>
<li>paramID 2 and 6 are for the output gate.</li>
<li>paramID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU paramID 0 to 2 refer to the weight matrix offset associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 3 are for the update gate.</li>
<li>paramID 1 and 4 are for the reset gate.</li>
<li>paramID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p>The output argument paramDesc is a previously created tensor descriptor that is populated to describe the memory layout of the parameter matrix. It is full packed and is used when calling to <a class="reference internal" href="#group__RNN_1ga91cb9f6b344118c642c75326607d08a2"><span class="std std-ref">miopenSetRNNLayerParam()</span></a></p>
<p>The argument layerParam should either be nullptr, or have device memory allocated to allow copying of the entire layer parameter matrix into it. If layerParam is nullptr then only the paramDesc is populated and returned. The size in bytes of the layer parameter matrix can be determined by using <a class="reference internal" href="#group__RNN_1gacf408b97337cd281aa7fadb8c430b8aa"><span class="std std-ref">miopenGetRNNLayerParamSize()</span></a>.</p>
<p>Note: When inputSkip mode is selected there is no input layer matrix operation, and therefore no associated memory. In this case <a class="reference internal" href="#group__RNN_1ga32a36f2568cb5f688768b782dcfd6b15"><span class="std std-ref">miopenGetRNNLayerParam()</span></a> will return a error status miopenStatusBadParm for input paramID associated with the input GEMM.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A tensor descriptor to the parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to memory containing parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramDesc</span></code>: Tensor descriptor for the fully packed output parameter tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">layerParam</span></code>: Pointer to the memory location of the parameter tensor (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnlayerbias">
<h2>miopenGetRNNLayerBias<a class="headerlink" href="#miopengetrnnlayerbias" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv321miopenGetRNNLayerBias14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPKvKi24miopenTensorDescriptor_tPv">
<span id="_CPPv221miopenGetRNNLayerBias14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPKvKi24miopenTensorDescriptor_tPv"></span><span id="miopenGetRNNLayerBias__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.miopenTensorDescriptor_t.voidCP.iC.miopenTensorDescriptor_t.voidP"></span><span class="target" id="group__RNN_1ga2b6e7249805a8af856e20a3aeeedf1d6"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNLayerBias</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, miopenTensorDescriptor_t <em>wDesc</em>, <em class="property">const</em> void *<em>w</em>, <em class="property">const</em> int <em>biasID</em>, miopenTensorDescriptor_t <em>biasDesc</em>, void *<em>layerBias</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv321miopenGetRNNLayerBias14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPKvKi24miopenTensorDescriptor_tPv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Gets a bias for a specific layer in an RNN stack. </p>
<p>This function retrieves the bias data for a specific layer and bias ID and copies the data into previously allocated device memory.</p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, biasID == 0 retrieves the bias associated with the in input GEMM, while biasID == 1 retrieves the bias associated with the hidden state GEMM.</p>
<p>For miopenLSTM biasID 0 to 3 refer to the biases associated with the input GEMM, 4-7 are associated with biases associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 4 are for the input gate.</li>
<li>biasID 1 and 5 are for the forget gate.</li>
<li>biasID 2 and 6 are for the output gate.</li>
<li>biasID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU biasID 0 to 2 refer to the biases associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 3 are for the update gate.</li>
<li>biasID 1 and 4 are for the reset gate.</li>
<li>biasID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p>The output argument biasDesc is a previously created tensor descriptor that is populated to describe the memory layout of the bias. It is full packed and is used when calling to <a class="reference internal" href="#group__RNN_1ga14e06851c57c2447746129ac5ee22f9d"><span class="std std-ref">miopenSetRNNLayerBias()</span></a></p>
<p>The argument layerBias should either be nullptr, or have device memory allocated to allow copying of the entire layer bias into it. If layerBias is nullptr then only the biasDesc is populated and returned. The size in bytes of the layer bias can be determined by using <a class="reference internal" href="#group__RNN_1ga38d7e7c6db99645c92b608f7117e50d7"><span class="std std-ref">miopenGetRNNLayerBiasSize()</span></a>.</p>
<p>Note: When inputSkip mode is selected there is no input layer matrix operation, and therefore no associated memory. In this case <a class="reference internal" href="#group__RNN_1ga2b6e7249805a8af856e20a3aeeedf1d6"><span class="std std-ref">miopenGetRNNLayerBias()</span></a> will return a error status miopenStatusBadParm for input biasID associated with the input GEMM.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A tensor descriptor to the parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to memory containing parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasDesc</span></code>: Descriptor of the parameter tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">layerBias</span></code>: Pointer to the memory location of the bias tensor (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopensetrnnlayerparam">
<h2>miopenSetRNNLayerParam<a class="headerlink" href="#miopensetrnnlayerparam" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv322miopenSetRNNLayerParam14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPvKi24miopenTensorDescriptor_tPKv">
<span id="_CPPv222miopenSetRNNLayerParam14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPvKi24miopenTensorDescriptor_tPKv"></span><span id="miopenSetRNNLayerParam__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.miopenTensorDescriptor_t.voidP.iC.miopenTensorDescriptor_t.voidCP"></span><span class="target" id="group__RNN_1ga91cb9f6b344118c642c75326607d08a2"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenSetRNNLayerParam</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, miopenTensorDescriptor_t <em>wDesc</em>, void *<em>w</em>, <em class="property">const</em> int <em>paramID</em>, miopenTensorDescriptor_t <em>paramDesc</em>, <em class="property">const</em> void *<em>layerParam</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv322miopenSetRNNLayerParam14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPvKi24miopenTensorDescriptor_tPKv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets a weight matrix for a specific layer in an RNN stack. </p>
<p>This function sets the weight matrix data for a specific layer and parameter ID.</p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, paramID == 0 sets the weight matrix associated with the in input GEMM, while paramID == 1 sets the weight matrix associated with the hidden state GEMM.</p>
<p>For miopenLSTM paramID 0 to 3 refer to the weight matrices associated with the input GEMM, 4-7 are associated with matrices associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 4 are for the input gate.</li>
<li>paramID 1 and 5 are for the forget gate.</li>
<li>paramID 2 and 6 are for the output gate.</li>
<li>paramID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU paramID 0 to 2 refer to the weight matrix offset associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 3 are for the update gate.</li>
<li>paramID 1 and 4 are for the reset gate.</li>
<li>paramID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p>The input argument paramDesc is a previously populated tensor descriptor typically by first calling <a class="reference internal" href="#group__RNN_1ga32a36f2568cb5f688768b782dcfd6b15"><span class="std std-ref">miopenGetRNNLayerParam()</span></a>.</p>
<p>Note: When inputSkip mode is selected there is no input layer matrix operation, and therefore no associated memory. In this case <a class="reference internal" href="#group__RNN_1ga91cb9f6b344118c642c75326607d08a2"><span class="std std-ref">miopenSetRNNLayerParam()</span></a> will return a error status miopenStatusBadParm for input paramID associated with the input GEMM.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A tensor descriptor to the parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to memory containing parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramDesc</span></code>: Descriptor of the parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layerParam</span></code>: Pointer to the memory location of the parameter tensor (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopensetrnnlayerbias">
<h2>miopenSetRNNLayerBias<a class="headerlink" href="#miopensetrnnlayerbias" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv321miopenSetRNNLayerBias14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPvKi24miopenTensorDescriptor_tPKv">
<span id="_CPPv221miopenSetRNNLayerBias14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPvKi24miopenTensorDescriptor_tPKv"></span><span id="miopenSetRNNLayerBias__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.miopenTensorDescriptor_t.voidP.iC.miopenTensorDescriptor_t.voidCP"></span><span class="target" id="group__RNN_1ga14e06851c57c2447746129ac5ee22f9d"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenSetRNNLayerBias</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, miopenTensorDescriptor_t <em>wDesc</em>, void *<em>w</em>, <em class="property">const</em> int <em>biasID</em>, miopenTensorDescriptor_t <em>biasDesc</em>, <em class="property">const</em> void *<em>layerBias</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv321miopenSetRNNLayerBias14miopenHandle_t21miopenRNNDescriptor_tKi24miopenTensorDescriptor_t24miopenTensorDescriptor_tPvKi24miopenTensorDescriptor_tPKv" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Sets a bias for a specific layer in an RNN stack. </p>
<p>This function sets the bias data for a specific layer and bias ID.</p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, biasID == 0 retrieves the weight matrix associated with the in input GEMM, while biasID == 1 retrieves the bias associated with the hidden state GEMM.</p>
<p>For miopenLSTM biasID 0 to 3 refer to the biases associated with the input GEMM, 4-7 are associated with the biases associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 4 are for the input gate.</li>
<li>biasID 1 and 5 are for the forget gate.</li>
<li>biasID 2 and 6 are for the output gate.</li>
<li>biasID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU biasID 0 to 2 refer to the biases associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 3 are for the update gate.</li>
<li>biasID 1 and 4 are for the reset gate.</li>
<li>biasID 2 and 5 are for the new new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p>The input argument biasDesc is a previously populated tensor descriptor typically by first calling miopenGetRNNLayeBias().</p>
<p>Note: When inputSkip mode is selected there is no input layer matrix operation, and therefore no associated memory. In this case miopenSetRNNLayerBias will return a error status miopenStatusBadParm for input biasID associated with the input GEMM.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A tensor descriptor to the bias tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to memory containing bias tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasID</span></code>: ID of the internal bias tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasDesc</span></code>: Descriptor of the bias tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">layerBias</span></code>: Pointer to the memory location of the bias tensor (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnlayerparamoffset">
<h2>miopenGetRNNLayerParamOffset<a class="headerlink" href="#miopengetrnnlayerparamoffset" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv328miopenGetRNNLayerParamOffset21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKi24miopenTensorDescriptor_tP6size_t">
<span id="_CPPv228miopenGetRNNLayerParamOffset21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKi24miopenTensorDescriptor_tP6size_t"></span><span id="miopenGetRNNLayerParamOffset__miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.iC.miopenTensorDescriptor_t.sP"></span><span class="target" id="group__RNN_1ga83ba3480acd40b0052ccb02fd9919512"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNLayerParamOffset</code><span class="sig-paren">(</span>miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, <em class="property">const</em> int <em>paramID</em>, miopenTensorDescriptor_t <em>paramDesc</em>, size_t *<em>layerParamOffset</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv328miopenGetRNNLayerParamOffset21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKi24miopenTensorDescriptor_tP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Gets an index offset for a specific weight matrix for a layer in the RNN stack. </p>
<p>This function retrieves the index offset for a weight matrix in a layer.</p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, paramID == 0 retrieves the weight matrix offset associated with the in input GEMM, while paramID == 1 retrieves the weight matrix offset associated with the hidden state GEMM.</p>
<p>For miopenLSTM paramID 0 to 3 refer to the weight matrix offsets associated with the input GEMM, 4-7 are associated with matrix offset associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 4 are for the input gate.</li>
<li>paramID 1 and 5 are for the forget gate.</li>
<li>paramID 2 and 6 are for the output gate.</li>
<li>paramID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU paramID 0 to 2 refer to the weight matrix offset associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>paramID 0 and 3 are for the update gate.</li>
<li>paramID 1 and 4 are for the reset gate.</li>
<li>paramID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p>The output argument paramDesc is a previously created tensor descriptor that is populated to describe the memory layout of the parameter matrix. It is full packed and is used when calling to <a class="reference internal" href="#group__RNN_1ga91cb9f6b344118c642c75326607d08a2"><span class="std std-ref">miopenSetRNNLayerParam()</span></a>.</p>
<p>The argument layerParamOffset should either be nullptr, or an address to place the offset. If layerParamOffset is nullptr then only the paramDesc is populated and returned.</p>
<p>Note: When inputSkip mode is selected there is no input layer matrix operation, and therefore no associated memory. In this case <a class="reference internal" href="#group__RNN_1ga83ba3480acd40b0052ccb02fd9919512"><span class="std std-ref">miopenGetRNNLayerParamOffset()</span></a> will return a error status miopenStatusBadParm for input paramID associated with the input GEMM.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">paramDesc</span></code>: Tensor descriptor for the fully packed output parameter tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">layerParamOffset</span></code>: Location for the parameter offset (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopengetrnnlayerbiasoffset">
<h2>miopenGetRNNLayerBiasOffset<a class="headerlink" href="#miopengetrnnlayerbiasoffset" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv327miopenGetRNNLayerBiasOffset21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKi24miopenTensorDescriptor_tP6size_t">
<span id="_CPPv227miopenGetRNNLayerBiasOffset21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKi24miopenTensorDescriptor_tP6size_t"></span><span id="miopenGetRNNLayerBiasOffset__miopenRNNDescriptor_t.iC.miopenTensorDescriptor_t.iC.miopenTensorDescriptor_t.sP"></span><span class="target" id="group__RNN_1gaa41e347aa39a21df41d8ee46e5976e32"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenGetRNNLayerBiasOffset</code><span class="sig-paren">(</span>miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>layer</em>, miopenTensorDescriptor_t <em>xDesc</em>, <em class="property">const</em> int <em>biasID</em>, miopenTensorDescriptor_t <em>biasDesc</em>, size_t *<em>layerBiasOffset</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv327miopenGetRNNLayerBiasOffset21miopenRNNDescriptor_tKi24miopenTensorDescriptor_tKi24miopenTensorDescriptor_tP6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Gets a bias index offset for a specific layer in an RNN stack. </p>
<p>This function retrieves the bias index offset for a specific layer and bias ID.</p>
<p>For RNN vanilla miopenRNNRELU and miopenRNNTANH, biasID == 0 retrieves the bias associated with the in input GEMM, while biasID == 1 retrieves the weight matrix associated with the hidden state GEMM.</p>
<p>For miopenLSTM biasID 0 to 3 refer to the bias offset associated with the input GEMM, 4-7 are the bias offsets associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 4 are for the input gate.</li>
<li>biasID 1 and 5 are for the forget gate.</li>
<li>biasID 2 and 6 are for the output gate.</li>
<li>biasID 3 and 7 are for the new memory gate.</li>
</ul>
</p>
<p>For miopenGRU biasID 0 to 2 refer to the biases associated with the input GEMM, while 3 through 5 are associated with the hidden state GEMM.</p>
<p><ul class="simple">
<li>biasID 0 and 3 are for the update gate.</li>
<li>biasID 1 and 4 are for the reset gate.</li>
<li>biasID 2 and 5 are for the new memory gate.</li>
</ul>
</p>
<p>For bi-directional RNNs the backwards in time direction is numbered as the layer directly after the forward in time direction.</p>
<p>The output argument biasDesc is a previously created tensor descriptor that is populated to describe the memory layout of the bias. It is full packed and is used when calling to <a class="reference internal" href="#group__RNN_1ga14e06851c57c2447746129ac5ee22f9d"><span class="std std-ref">miopenSetRNNLayerBias()</span></a></p>
<p>The argument layerBiasOffset should either be nullptr, or point to an output address. If layerBias is nullptr then only the biasDesc is populated and returned.</p>
<p>Note: When inputSkip mode is selected there is no input layer matrix operation, and therefore no associated memory. In this case <a class="reference internal" href="#group__RNN_1gaa41e347aa39a21df41d8ee46e5976e32"><span class="std std-ref">miopenGetRNNLayerBiasOffset()</span></a> will return a error status miopenStatusBadParm for input biasID associated with the input GEMM.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">layer</span></code>: The layer number in the RNN stack (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: A tensor descriptor to input (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasID</span></code>: ID of the internal parameter tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">biasDesc</span></code>: Descriptor of the parameter tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">layerBiasOffset</span></code>: Pointer to the memory location of the bias tensor (output) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopenrnnforwardtraining">
<h2>miopenRNNForwardTraining<a class="headerlink" href="#miopenrnnforwardtraining" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv324miopenRNNForwardTraining14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_tPv6size_t">
<span id="_CPPv224miopenRNNForwardTraining14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_tPv6size_t"></span><span id="miopenRNNForwardTraining__miopenHandle_t.miopenRNNDescriptor_tC.iC.miopenTensorDescriptor_tCP.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tCP.voidP.miopenTensorDescriptor_tC.voidP.miopenTensorDescriptor_tC.voidP.voidP.s.voidP.s"></span><span class="target" id="group__RNN_1gaaa6979bb1e5d1e682219f22168608d3e"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenRNNForwardTraining</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, <em class="property">const</em> miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>sequenceLen</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>xDesc</em>, <em class="property">const</em> void *<em>x</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>hxDesc</em>, <em class="property">const</em> void *<em>hx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>cxDesc</em>, <em class="property">const</em> void *<em>cx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>wDesc</em>, <em class="property">const</em> void *<em>w</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>yDesc</em>, void *<em>y</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>hyDesc</em>, void *<em>hy</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>cyDesc</em>, void *<em>cy</em>, void *<em>workSpace</em>, size_t <em>workSpaceNumBytes</em>, void *<em>reserveSpace</em>, size_t <em>reserveSpaceNumBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv324miopenRNNForwardTraining14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_tPv6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Execute forward training for recurrent layer. </p>
<p>Interface for executing the forward training pass on a RNN.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">sequenceLen</span></code>: Temporal iterations to unroll (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">x</span></code>: Pointer to input tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hxDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hx</span></code>: Pointer to the hidden layer input tensor. If hx is NULL, then the initial hidden state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cxDesc</span></code>: A cell tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cx</span></code>: Pointer to the cell layer input tensor. If cx is NULL, then the initial cell state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A weights tensor descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to input weights tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">yDesc</span></code>: An array of fully packed tensor descriptors associated with the output from each time step. The first dimension of the tensor descriptors must equal the first dimension of the first descriptor (batch size) in the xDesc tensor array. The second dimension of the element of the descriptor array depends on the direction mode selected. If the direction mode is unidirectional, the second dimension is the hiddenSize. If direction mode is bidirectional the second dimension is twice the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">y</span></code>: Pointer to output tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">hyDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hy</span></code>: Pointer to the hidden layer output tensor. If hy is NULL, then the final hidden state will not be saved. (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">cyDesc</span></code>: A cell tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cy</span></code>: Pointer to the cell layer output tensor. If hy is NULL, then the final cell state will not be saved. (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpace</span></code>: Pointer to memory allocated for forward training (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpaceNumBytes</span></code>: Number of allocated bytes in memory for the workspace (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">reserveSpace</span></code>: Pointer to memory allocated for random states (input / output) </li>
<li><code class="docutils literal notranslate"><span class="pre">reserveSpaceNumBytes</span></code>: Number of allocated bytes in memory for use in the forward (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopenrnnbackwarddata">
<h2>miopenRNNBackwardData<a class="headerlink" href="#miopenrnnbackwarddata" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv321miopenRNNBackwardData14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_tPv6size_t">
<span id="_CPPv221miopenRNNBackwardData14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_tPv6size_t"></span><span id="miopenRNNBackwardData__miopenHandle_t.miopenRNNDescriptor_tC.iC.miopenTensorDescriptor_tCP.voidCP.miopenTensorDescriptor_tCP.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tCP.voidP.miopenTensorDescriptor_tC.voidP.miopenTensorDescriptor_tC.voidP.voidP.s.voidP.s"></span><span class="target" id="group__RNN_1ga7e8ebd33a6381187b0ad16ea7e1a8863"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenRNNBackwardData</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, <em class="property">const</em> miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>sequenceLen</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>yDesc</em>, <em class="property">const</em> void *<em>y</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>dyDesc</em>, <em class="property">const</em> void *<em>dy</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>dhyDesc</em>, <em class="property">const</em> void *<em>dhy</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>dcyDesc</em>, <em class="property">const</em> void *<em>dcy</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>wDesc</em>, <em class="property">const</em> void *<em>w</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>hxDesc</em>, <em class="property">const</em> void *<em>hx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>cxDesc</em>, <em class="property">const</em> void *<em>cx</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>dxDesc</em>, void *<em>dx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>dhxDesc</em>, void *<em>dhx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>dcxDesc</em>, void *<em>dcx</em>, void *<em>workSpace</em>, size_t <em>workSpaceNumBytes</em>, void *<em>reserveSpace</em>, size_t <em>reserveSpaceNumBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv321miopenRNNBackwardData14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_tPv6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Execute forward training for recurrent layer. </p>
<p>Interface for executing the forward training pass on a RNN.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">sequenceLen</span></code>: Temporal iterations to unroll (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">yDesc</span></code>: An array of tensor descriptors (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">y</span></code>: Pointer to input tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dyDesc</span></code>: An array of fully packed tensor descriptors associated with the output from each time step. The first dimension of the tensor descriptors must equal the first dimension of the first descriptor (batch size) in the xDesc tensor array. The second dimension of the element of the descriptor array depends on the direction mode selected. If the direction mode is unidirectional, the second dimension is the hiddenSize. If direction mode is bidirectional the second dimension is twice the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dy</span></code>: Pointer to the hidden layer input tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dhyDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dhy</span></code>: Pointer to the cell layer input tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dcyDesc</span></code>: A cell tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dcy</span></code>: Pointer to the cell layer input tensor. If dcy is NULL, then the initial delta cell state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A weights tensor descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to input weights tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hxDesc</span></code>: An input hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hx</span></code>: Pointer to the hidden layer input tensor. If hx is NULL, then the initial hidden state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cxDesc</span></code>: A input cell tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cx</span></code>: Pointer to the hidden layer input tensor. If cx is NULL, then the initial cell state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dxDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dx</span></code>: Pointer to the cell layer output tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">dhxDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dhx</span></code>: Pointer to the delta hidden layer output tensor. If dhx is NULL the hidden gradient will not ouput. (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">dcxDesc</span></code>: A tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dcx</span></code>: Pointer to the cell layer output tensor. If dcx is NULL the cell gradient will not ouput. (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpace</span></code>: Pointer to memory allocated for forward training (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpaceNumBytes</span></code>: Number of allocated bytes in memory for the workspace (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">reserveSpace</span></code>: Pointer to memory allocated for random states (input / output) </li>
<li><code class="docutils literal notranslate"><span class="pre">reserveSpaceNumBytes</span></code>: Number of allocated bytes in memory for use in the forward (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopenrnnbackwardweights">
<h2>miopenRNNBackwardWeights<a class="headerlink" href="#miopenrnnbackwardweights" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv324miopenRNNBackwardWeights14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPvPv6size_tPKv6size_t">
<span id="_CPPv224miopenRNNBackwardWeights14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPvPv6size_tPKv6size_t"></span><span id="miopenRNNBackwardWeights__miopenHandle_t.miopenRNNDescriptor_tC.iC.miopenTensorDescriptor_tCP.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tCP.voidCP.miopenTensorDescriptor_tC.voidP.voidP.s.voidCP.s"></span><span class="target" id="group__RNN_1ga872ab55d610ebb28835641f7562c9ecb"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenRNNBackwardWeights</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, <em class="property">const</em> miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>sequenceLen</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>xDesc</em>, <em class="property">const</em> void *<em>x</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>hxDesc</em>, <em class="property">const</em> void *<em>hx</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>yDesc</em>, <em class="property">const</em> void *<em>y</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>dwDesc</em>, void *<em>dw</em>, void *<em>workSpace</em>, size_t <em>workSpaceNumBytes</em>, <em class="property">const</em> void *<em>reserveSpace</em>, size_t <em>reserveSpaceNumBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv324miopenRNNBackwardWeights14miopenHandle_tK21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPvPv6size_tPKv6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Execute forward training for recurrent layer. </p>
<p>Interface for executing the forward training pass on a RNN.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">sequenceLen</span></code>: Temporal iterations to unroll (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">x</span></code>: Pointer to input tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hxDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hx</span></code>: Pointer to the hidden layer input tensor. If hx is NULL, then the initial hidden state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">yDesc</span></code>: An array of fully packed tensor descriptors associated with the output from each time step. The first dimension of the tensor descriptors must equal the first dimension of the first descriptor (batch size) in the xDesc tensor array. The second dimension of the element of the descriptor array depends on the direction mode selected. If the direction mode is unidirectional, the second dimension is the hiddenSize. If direction mode is bidirectional the second dimension is twice the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">y</span></code>: Pointer to the output tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dwDesc</span></code>: A weights tensor descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">dw</span></code>: Pointer to input weights tensor (input / output) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpace</span></code>: Pointer to memory allocated for forward training (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpaceNumBytes</span></code>: Number of allocated bytes in memory for the workspace (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">reserveSpace</span></code>: Pointer to memory allocated for random states (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">reserveSpaceNumBytes</span></code>: Number of allocated bytes in memory for use in the forward (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
<div class="section" id="miopenrnnforwardinference">
<h2>miopenRNNForwardInference<a class="headerlink" href="#miopenrnnforwardinference" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="_CPPv325miopenRNNForwardInference14miopenHandle_t21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_t">
<span id="_CPPv225miopenRNNForwardInference14miopenHandle_t21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_t"></span><span id="miopenRNNForwardInference__miopenHandle_t.miopenRNNDescriptor_t.iC.miopenTensorDescriptor_tCP.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tC.voidCP.miopenTensorDescriptor_tCP.voidP.miopenTensorDescriptor_tC.voidP.miopenTensorDescriptor_tC.voidP.voidP.s"></span><span class="target" id="group__RNN_1ga104b48106a4fde5597f01893cf3df3d5"></span> <a class="reference internal" href="handle.html#_CPPv314miopenStatus_t" title="miopenStatus_t">miopenStatus_t</a> <code class="descname">miopenRNNForwardInference</code><span class="sig-paren">(</span>miopenHandle_t <em>handle</em>, miopenRNNDescriptor_t <em>rnnDesc</em>, <em class="property">const</em> int <em>sequenceLen</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>xDesc</em>, <em class="property">const</em> void *<em>x</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>hxDesc</em>, <em class="property">const</em> void *<em>hx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>cxDesc</em>, <em class="property">const</em> void *<em>cx</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>wDesc</em>, <em class="property">const</em> void *<em>w</em>, <em class="property">const</em> miopenTensorDescriptor_t *<em>yDesc</em>, void *<em>y</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>hyDesc</em>, void *<em>hy</em>, <em class="property">const</em> miopenTensorDescriptor_t <em>cyDesc</em>, void *<em>cy</em>, void *<em>workSpace</em>, size_t <em>workSpaceNumBytes</em><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv325miopenRNNForwardInference14miopenHandle_t21miopenRNNDescriptor_tKiPK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvK24miopenTensorDescriptor_tPKvPK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvK24miopenTensorDescriptor_tPvPv6size_t" title="Permalink to this definition">¶</a><br /></dt>
<dd><p>Execute forward inference for RNN layer. </p>
<p>Interface for executing the forward inference pass on a RNN.</p>
<p><dl class="docutils">
<dt><strong>Return</strong></dt>
<dd>miopenStatus_t </dd>
<dt><strong>Parameters</strong></dt>
<dd><ul class="breatheparameterlist first last simple">
<li><code class="docutils literal notranslate"><span class="pre">handle</span></code>: MIOpen handle (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">rnnDesc</span></code>: RNN layer descriptor type (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">sequenceLen</span></code>: Temporal iterations to unroll (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">xDesc</span></code>: An array of tensor descriptors. These are the input descriptors to each time step. The first dimension of each descriptor is the batch size and may decrease from element n to element n+1 and not increase in size. The second dimension is the same for all descriptors in the array and is the input vector length. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">x</span></code>: Pointer to input tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hxDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hx</span></code>: Pointer to the hidden layer input tensor. If hx is NULL, then the initial hidden state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cxDesc</span></code>: A cell tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cx</span></code>: Pointer to the cell layer input tensor. If cx is NULL, then the initial cell state will be zero initialized. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">wDesc</span></code>: A weights tensor descriptor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">w</span></code>: Pointer to input weights tensor (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">yDesc</span></code>: An array of fully packed tensor descriptors associated with the output from each time step. The first dimension of the tensor descriptors must equal the first dimension of the first descriptor (batch size) in the xDesc tensor array. The second dimension of the element of the descriptor array depends on the direction mode selected. If the direction mode is unidirectional, the second dimension is the hiddenSize. If direction mode is bidirectional the second dimension is twice the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">y</span></code>: Pointer to output tensor (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">hyDesc</span></code>: A hidden tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">hy</span></code>: Pointer to the hidden layer output tensor. If hy is NULL, then the final hidden state will not be saved. (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">cyDesc</span></code>: A output cell tensor descriptor that has as its first dimension of the number of layers if the direction mode is unidirectional and twice the number of layers if the direction mode is bidirectional. The second dimension of the descriptor must equal the largest first dimension of the xDesc tensor descriptor array. The third dimension equals the hiddenSize. (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">cy</span></code>: Pointer to the cell layer output tensor. If cy is NULL, then the final cell state will not be saved. (output) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpace</span></code>: Pointer to memory allocated for forward training (input) </li>
<li><code class="docutils literal notranslate"><span class="pre">workSpaceNumBytes</span></code>: Number of allocated bytes in memory for the workspace (input) </li>
</ul>
</dd>
</dl>
</p>
</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="batchnorm.html" class="btn btn-neutral float-right" title="Batch Normalization Layer" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="convolution.html" class="btn btn-neutral" title="Convolutional Layer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Advanced Micro Devices, Inc.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'2.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>